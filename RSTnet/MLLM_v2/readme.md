## Multi-modal LLM (speech-text foundation models)
In this part, we provide the training details of speech-text foundation models. 
We provide a moshi-style pre-training code.

## How to start it?

### Step 0: refer to litgpt https://github.com/Lightning-AI/litgpt/ to download the desired LLM checkpoints

### Step 1: refer to egs/pretraining, and check the extract_token.sh for data preprocessing

### Step 2: refer to egs/pretraining, and check the run.sh for model pre-training

### Step 3: refer to egs/pretraining, and check the infer.sh for inference


