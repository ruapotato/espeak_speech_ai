#!/usr/bin/env python3
"""
Decoder for speech synthesis from LM-generated tokens.
Converts tokens generated by the fine-tuned LLM back into audio.
"""

import torch
import numpy as np
import wave
import argparse
import re
import logging
from transformers import MimiModel, AutoFeatureExtractor
from pathlib import Path

def setup_logging(verbosity=1):
    """Set up logging with the specified verbosity level"""
    log_levels = {
        0: logging.ERROR,
        1: logging.WARNING,
        2: logging.INFO,
        3: logging.DEBUG
    }
    level = log_levels.get(verbosity, logging.INFO)
    
    logging.basicConfig(
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        level=level
    )
    return logging.getLogger("decoder")

def parse_lm_output(model_output_file):
    """
    Parse the LLM output to extract the audio tokens from all codebooks.
    Expects base filename pattern 'codebook_X_tokens.txt' where X is the codebook number.
    """
    logger = logging.getLogger("decoder")
    logger.info(f"Parsing LLM output from {model_output_file}")
    
    try:
        # Get the base path by removing "_X_tokens.txt" from the input filename
        base_path = model_output_file.replace("_tokens.txt", "")
        base_path = base_path.rsplit("_", 1)[0]  # Remove codebook number
        
        # Read tokens from all 32 codebook files
        all_tokens = []
        max_len = 0
        
        # First pass to get max length
        for i in range(32):
            filename = f"{base_path}_{i}_tokens.txt"
            logger.debug(f"Reading codebook file: {filename}")
            with open(filename, 'r') as f:
                tokens = list(map(int, f.read().strip().split()))
                max_len = max(max_len, len(tokens))
        
        logger.info(f"Found maximum token sequence length: {max_len}")
        
        # Second pass to create padded array
        all_tokens = np.zeros((32, max_len), dtype=np.int64)
        for i in range(32):
            filename = f"{base_path}_{i}_tokens.txt"
            with open(filename, 'r') as f:
                tokens = list(map(int, f.read().strip().split()))
                all_tokens[i, :len(tokens)] = tokens
        
        logger.info(f"Successfully loaded tokens from all codebooks")
        logger.info(f"Token array shape: {all_tokens.shape}")
        
        return all_tokens
    
    except Exception as e:
        logger.error(f"Error parsing model output: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None



def normalize_audio(audio_data, target_db=-16):
    """Normalize audio to target dB level"""
    logger = logging.getLogger("decoder")
    
    # Calculate current RMS amplitude
    rms = np.sqrt(np.mean(audio_data**2))
    
    if rms < 1e-6:  # Avoid division by zero or very low values
        logger.warning("Audio signal is too quiet, normalization might not be effective")
        return audio_data
    
    # Convert target dB to linear scale
    target_rms = 10**(target_db/20)
    
    # Calculate gain needed
    gain = target_rms / rms
    
    logger.info(f"Normalizing audio with gain: {gain:.2f}")
    
    # Apply gain
    normalized_audio = audio_data * gain
    
    # Clip to ensure we don't exceed [-1, 1]
    normalized_audio = np.clip(normalized_audio, -1.0, 1.0)
    
    return normalized_audio

def main():
    parser = argparse.ArgumentParser(description="Convert LLM-generated audio tokens back to speech")
    parser.add_argument("--model_output", type=str, required=True,
                      help="Path to any codebook token file (e.g., codebook_0_tokens.txt)")
    parser.add_argument("--output_file", type=str, default="output.wav",
                      help="Path for output audio file")
    parser.add_argument("--verbosity", type=int, default=1, choices=[0, 1, 2, 3],
                      help="Verbosity level (0-3)")
    parser.add_argument("--apply_normalization", action="store_true",
                      help="Normalize output audio")
    parser.add_argument("--target_db", type=float, default=-16,
                      help="Target dB level for normalization")
    
    args = parser.parse_args()
    logger = setup_logging(args.verbosity)
    
    # Parse the model output to get tokens
    all_tokens = parse_lm_output(args.model_output)
    if all_tokens is None:
        return 1
    
    # Load Mimi model for decoding
    logger.info("Loading Mimi model")
    try:
        model = MimiModel.from_pretrained("kyutai/mimi")
        feature_extractor = AutoFeatureExtractor.from_pretrained("kyutai/mimi")
    except Exception as e:
        logger.error(f"Error loading Mimi model: {e}")
        return 1
    
    # Convert tokens to tensor format expected by Mimi
    logger.info("Preparing tokens for decoding")
    tokens_tensor = torch.tensor(all_tokens).unsqueeze(0)  # Add batch dimension
    
    # Decode tokens back to audio
    logger.info("Decoding tokens to audio waveform")
    try:
        with torch.no_grad():
            decoded_audio = model.decode(tokens_tensor)[0].numpy()
        
        logger.info(f"Generated audio length: {len(decoded_audio)} samples")
        
        # Apply normalization if requested
        if args.apply_normalization:
            decoded_audio = normalize_audio(decoded_audio, args.target_db)
        
        # Save the audio to WAV file
        logger.info(f"Saving audio to {args.output_file}")
        
        # Convert to int16
        decoded_int16 = (np.clip(decoded_audio, -1.0, 1.0) * 32767).astype(np.int16)
        
        with wave.open(args.output_file, 'wb') as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)  # 2 bytes for int16
            wf.setframerate(feature_extractor.sampling_rate)
            wf.writeframes(decoded_int16.tobytes())
        
        logger.info("Audio generation complete")
        return 0
        
    except Exception as e:
        logger.error(f"Error decoding audio: {e}")
        return 1
if __name__ == "__main__":
    exit(main())

